import sys
from os import system, listdir, path
import pyfits
from scipy import loadtxt, sqrt, median, array, where, zeros, cos, hypot
import time
import ephem
import pickle
import socket


####    Declare the query_field and query_chip. This defines which stack of 
####    images we are coadding.
if len(sys.argv) != 3:
    print "Improper usage. As arguments, supply the field number and chip ID."
    print "Ex: coadd_images.py 100 C13"
    sys.exit()

query_field = sys.argv[1]
query_chip = sys.argv[2]
# query_field = "100"
# query_chip = "C13"

print "Starting coaddition of " + query_field + "-" + query_chip + "."
start_time = time.time()

""" Local laptop testing and development modification. """
if socket.gethostname() == "Christopher-Kleins-MacBook-Pro.local" or           \
    socket.gethostname()[:8] == "airbears":
    coadd_directory = "/Users/cklein/Desktop/Ongoing_Research/DECam_Science_Verification/create_relative_coadds/"
    #### Define the paths to the sex and swarp bins on my laptop.
    SEXCMD = "sex"
    SWARPCMD = "swarp"
else:
    coadd_directory = "/global/scratch2/sd/cenko/DES/Jan2014/coadds/"
    #### Define the paths to the sex and swarp bins on carver.
    SEXCMD = "/project/projectdirs/dessn/local/carver/bin/sex"
    SWARPCMD = "/project/projectdirs/dessn/local/carver/bin/swarp"
""" End of local laptop testing and development modification. """


####    Create a list of the paths on NERSC to all the reduced images.
####    Most of this work was done in creating the "imagefile_dict.p" 
####    pickled dictionary file.
imagefile_dict = pickle.load( open( "imagefile_dict.p", "rb" ) )
# The field and chip will change with each run. 45 fields and 61 chips total 
# means that this "coadd_images.py" script will run 2745 times.
image_path_list = imagefile_dict[query_field][query_chip]


""" Local laptop testing and development modification. """
if socket.gethostname() == "Christopher-Kleins-MacBook-Pro.local" or           \
    socket.gethostname()[:8] == "airbears":
    for n in range(len(image_path_list)):
        image_path_list[n] = "/Volumes/Extra_HDD/Research/DECam_Data" +        \
            image_path_list[n]
""" End of local laptop testing and development modification. """


####    Create a single mask image for the whole stack of input images.
weightmap_image = image_path_list[0][:image_path_list[0].rfind("/done")] +     \
    "/done/zband_weightmaps/zband_" + query_chip + "_weightmap.fits"
if not path.isfile(weightmap_image):
    flat_image = image_path_list[0][:image_path_list[0].rfind("/")] + "/flat.fits"
    mask_image = image_path_list[0][:image_path_list[0].rfind("/")] + "/mask.fits"
    flat_data = pyfits.open(flat_image)[0].data
    mask_data = pyfits.open(mask_image)[0].data
    weightmap_data = flat_data*mask_data
    # de-weight the top 66 pixels
    weightmap_data[-66:,:] = 0
    # de-weight the bottom 66 pixels
    weightmap_data[:66,:] = 0
    # de-weight the left 29 pixels
    weightmap_data[:,:29] = 0
    # de-weight the right 29 pixels
    weightmap_data[:,-29:] = 0
    output_hdu = pyfits.PrimaryHDU(weightmap_data)
    output_hdulist = pyfits.HDUList([output_hdu])
    output_hdulist.writeto(weightmap_image)


####    Create more-accurate WCS relative alignment with SCAMP.
scamp_input_path_list = []
for image_path in image_path_list:
    system(SEXCMD + " " + image_path + " -c decam_rrl_detect.sex" +            
        " -WEIGHT_IMAGE " + weightmap_image +                                  
        " -CATALOG_NAME " + image_path.replace(".fits", ".ldac") + 
        " -PARAMETERS_NAME decam_rrl_align.param")
#     system("psfex " + image_path.replace(".fits", ".ldac") +                   
#         " -c decam_rrl.psfex")
#     system(SEXCMD + " " + image_path + " -c decam_rrl_align.sex" +             
#         " -CATALOG_NAME " + image_path.replace(".fits", ".ldac") +             
#         " -WEIGHT_IMAGE " + weightmap_image +                                  
#         " -PSF_NAME " + image_path.replace(".fits", ".psf"))
    scamp_input_path_list.append(image_path.replace(".fits", ".ldac"))
scamp_catalog_string = ""
for catalog_path in scamp_input_path_list:
    scamp_catalog_string += catalog_path + " "
scamp_catalog_string = scamp_catalog_string.rstrip()
missfits_image_string = ""
for image_path in image_path_list:
    missfits_image_string += image_path + " "
missfits_image_string = missfits_image_string.rstrip()
system("scamp -c decam_rrl_align.scamp " + scamp_catalog_string)
system("missfits -c decam_rrl_align.missfits " + missfits_image_string)
system("rm " + missfits_image_string.replace(".fits", ".head"))


####    Now that images are better WCS-aligned, conduct PSF-photometry 
####    extraction by running sex with detection config, then psfex on the 
####    detection LDAC catalog, and then sex again with the psf config and using
####    the psf file generated by psfex. Also write out a region file for each
####    image.
for image_path in image_path_list:
    system(SEXCMD + " " + image_path + " -c decam_rrl_detect.sex" +            
        " -WEIGHT_IMAGE " + weightmap_image +                                  
        " -CATALOG_NAME " + image_path.replace(".fits", ".ldac"))
    system("psfex " + image_path.replace(".fits", ".ldac") +                   
        " -c decam_rrl.psfex")
    system(SEXCMD + " " + image_path + " -c decam_rrl_psf.sex" +               
        " -CATALOG_NAME " + image_path.replace(".fits", ".sexcat") +           
        " -WEIGHT_IMAGE " + weightmap_image +                                  
        " -PSF_NAME " + image_path.replace(".fits", ".psf"))
    # We write out a regions file from the sexcat for each image for the 
    # non-flagged, good detections.
    source_extractor_data = loadtxt(image_path.replace(".fits", ".sexcat"))
    # Keep only the non-flagged, good detections.
    #       source_extractor_data[:,29] is the FLAGS column
    #       source_extractor_data[:,5] is the FLUX_PSF column
    sex_ra = source_extractor_data[:,3][(source_extractor_data[:,29]==0) &     \
                                        (source_extractor_data[:,5]!=0)]
    sex_dec = source_extractor_data[:,4][(source_extractor_data[:,29]==0) &    \
                                         (source_extractor_data[:,5]!=0)]
    sex_mag_psf = source_extractor_data[:,21][(source_extractor_data[:,29]==0) \
                                            & (source_extractor_data[:,5]!=0)]
    output_region_file = file(image_path.replace(".fits", ".reg"), "w")
    output_region_file.write("global color=blue dashlist=8 3 width=1 " + 
        "font='helvetica 12 bold' select=1 highlite=1 dash=0 fixed=0 edit=1 " + 
        "move=1 delete=1 include=1 source=1\nfk5\n")
    for n in range(len(sex_ra)):
        star_ra = sex_ra[n]
        star_dec = sex_dec[n]
        star_psfmag_z = sex_mag_psf[n]
        output_region_file.write(('''circle(%f,%f,1") # color=blue ''' + 
            "width=1 text={MAG_PSF=%.3f}\n") % 
            (star_ra, star_dec, star_psfmag_z))
    output_region_file.close()


####    Final major step is to select the best candidate calibration stars in 
####    the combined coverage of all the filed-chip images.
# Define median Absolute Deviation clipping for input array of numbers.
def mad_clipping(input_data, sigma_clip_level, return_length=False):
    medval = median(input_data)
    sigma = 1.4826 * median(abs(medval - input_data))
    high_sigma_clip_limit = medval + sigma_clip_level * sigma
    low_sigma_clip_limit = medval - sigma_clip_level * sigma
    clipped_data = input_data[(input_data>(low_sigma_clip_limit)) &            \
                              (input_data<(high_sigma_clip_limit))]
    new_medval = median(clipped_data)
    new_sigma = 1.4826 * median(abs(medval - clipped_data))
    if return_length:
        return new_medval, new_sigma, len(clipped_data)
    else:
        return new_medval, new_sigma
min_epochs_limit = int(len(image_path_list) * (2.0/3.0))

light_curve_data = []
# Format of light_curve_data is [ra, dec, [photometry]]
# [photometry] is list of photometry for each image, order is
#   phot_cat, MAG_PSF, MAGERR_PSF, MAG_APER[20], MAGERR_APER[20]

fwhm_list = []

# This first pass creates the light_curve_data list structure and identifies the
# best epoch (which will be used as the relative zero-point) as the image with 
# the best fwhm. 
for image_path in image_path_list:
    phot_cat_path = image_path.replace(".fits", ".sexcat")
    data = loadtxt(phot_cat_path)

    # Strip out flagged data or data with zero FLUX_PSF
    data = data[(data[:,29]==0) & (data[:,5]!=0)]

    if len(data) < 23:
        continue

    # clip out the detections where the MAG_APER[20] and MAG_PSF disagree by 
    # more than 3-sigma outside the median residual
    aper_psf_resid = data[:,13]-data[:,21]
    resid_median, resid_sigma = mad_clipping(aper_psf_resid, 3)
    data = data[(aper_psf_resid<resid_median+3*resid_sigma) & 
                (aper_psf_resid>resid_median-3*resid_sigma)]

    if len(data) < 23:
        continue

    # Clip out detections with fwhm outside 3-sigma range
    fwhm_median, fwhm_sigma = mad_clipping(data[:,30], 3)
    data = data[(data[:,30]<fwhm_median+3*fwhm_sigma) & 
                (data[:,30]>fwhm_median-3*fwhm_sigma)]
    new_fwhm_median, new_fwhm_sigma = mad_clipping(data[:,30], 3)
    fwhm_list.append([new_fwhm_median, phot_cat_path])
    
    if len(data) < 23:
        continue
    
    for row in data:
        row_ra = row[3]
        row_dec = row[4]
        
        curve_coords_ra = zeros(len(light_curve_data))
        curve_coords_dec = zeros(len(light_curve_data))
        for n in range(len(light_curve_data)):
            curve_coords_ra[n] = light_curve_data[n][0]
            curve_coords_dec[n] = light_curve_data[n][1]
        
        dist_array = hypot(cos(row_dec*0.01745329252)*(row_ra-curve_coords_ra), 
                                    row_dec-curve_coords_dec)
        if len(dist_array) > 0:
            min_dist_index = where(dist_array==dist_array.min())[0][0]
            row_curve_separation = 206264.806247*(float(ephem.separation(
                    (light_curve_data[min_dist_index][0] * 0.01745329252, 
                    light_curve_data[min_dist_index][1] * 0.01745329252), 
                    (row_ra * 0.01745329252, row_dec * 0.01745329252))))
            if row_curve_separation < 2.0:
                curve_length = len(light_curve_data[min_dist_index][2])
                light_curve_data[min_dist_index][0] =                          \
                    light_curve_data[min_dist_index][0] *                      \
                        ((curve_length-1.0)/float(curve_length)) +             \
                            row_ra*((1.0)/float(curve_length))
                light_curve_data[min_dist_index][1] =                          \
                    light_curve_data[min_dist_index][1] *                      \
                        ((curve_length-1.0)/float(curve_length)) +             \
                            row_dec*((1.0)/float(curve_length))
                light_curve_data[min_dist_index][2].append([phot_cat_path, 
                                            row[21], row[22], row[13], row[20]])
            else:
                light_curve_data.append([row_ra, row_dec, 
                                [[phot_cat_path, 
                                row[21], row[22], row[13], row[20]]] ] )
        else:
            light_curve_data.append([row_ra, row_dec, 
                            [[phot_cat_path, 
                            row[21], row[22], row[13], row[20]]] ] )
fwhm_by_epoch = array(fwhm_list)
fwhm_array = fwhm_by_epoch[:,0].astype(float)

best_epoch_name = fwhm_by_epoch[where(fwhm_array == fwhm_array.min())[0][0], 1]

# These "izp" zeropoint dictionaries are intermediate. The are created before
# any low-quality epochs are rejected and before suspected variables are 
# rejected from the set of candidate calibrators. At the same time, we also 
# initialize the final "zp" zeropoint dictionaries which will be filled-in 
# later.
psf_photometry_izp_dict = {}
aper_photometry_izp_dict = {}
psf_photometry_zp_dict = {}
aper_photometry_zp_dict = {}
for image_path in image_path_list:
    phot_cat_path = image_path.replace(".fits", ".sexcat")
    psf_photometry_izp_dict[phot_cat_path] = []
    aper_photometry_izp_dict[phot_cat_path] = []
    psf_photometry_zp_dict[phot_cat_path] = []
    aper_photometry_zp_dict[phot_cat_path] = []

for curve_data in light_curve_data:
    # Enforce the requirement that the candidate calibrator be detected in at 
    # least min_epochs_limit images. Also require that it exist in the 
    # best_epoch's image.
    if (len(curve_data[2]) >= min_epochs_limit) and                            \
       (best_epoch_name in array(curve_data[2])[:,0]):
        ra = curve_data[0]
        dec = curve_data[1]
        curve = curve_data[2]
        # phot_cat, MAG_PSF, MAGERR_PSF, MAG_APER[20], MAGERR_APER[20]
        best_epoch_mag_psf = 0
        best_epoch_mag_aper = 0
        # Pluck out the MAG_PSF and MAG_APER[20] for the best_epoch
        for epoch in curve_data[2]:
            if epoch[0] == best_epoch_name:
                best_epoch_mag_psf = epoch[1]
                best_epoch_mag_aper = epoch[3]
        # Fill in the intermediate zeropoint dicts.
        for epoch in curve_data[2]:
            psf_photometry_izp_dict[epoch[0]].append(epoch[1] - 
                                                            best_epoch_mag_psf)
            aper_photometry_izp_dict[epoch[0]].append(epoch[3] - 
                                                            best_epoch_mag_aper)

# Collect the intermediate zeropoint data by epoch and calculate the median and 
# associated sigma.
epoch_psf_izp_list = []
epoch_aper_izp_list = []
for image_path in image_path_list:
    phot_cat_path = image_path.replace(".fits", ".sexcat")
    if phot_cat_path != best_epoch_name:
        psf_zp_array = array(psf_photometry_izp_dict[phot_cat_path])
        aper_zp_array = array(aper_photometry_izp_dict[phot_cat_path])
        psf_zp_medval, psf_zp_sigma = mad_clipping(psf_zp_array, 3)
        aper_zp_medval, aper_zp_sigma = mad_clipping(aper_zp_array, 3)
        epoch_psf_izp_list.append([phot_cat_path, 
                                   psf_zp_medval, psf_zp_sigma])
        epoch_aper_izp_list.append([phot_cat_path, 
                                    aper_zp_medval, aper_zp_sigma])
    else:
        epoch_psf_izp_list.append([phot_cat_path, 0, 0])
        epoch_aper_izp_list.append([phot_cat_path, 0, 0])

# Now create final zeropoint data by using the intermediate zeropoint values
# to identify candidate calibrators that are variable, and also by rejecting
# from the stack (and later coaddition) images that are of poor quality.
output_region_file = file(coadd_directory + query_field + "-" + query_chip +   \
                          "_candidate_calibrators.reg", "w")
output_region_file.write("global color=blue dashlist=8 3 width=1 " + 
    "font='helvetica 12 bold' select=1 highlite=1 dash=0 fixed=0 edit=1 " + 
    "move=1 delete=1 include=1 source=1\nfk5\n")

psf_mag_sigma_list = []
aper_mag_sigma_list = []
psf_mag_medval_list = []
aper_mag_medval_list = []
calibrator_ra = []
calibrator_dec = []
for curve_data in light_curve_data:
    if (len(curve_data[2]) >= min_epochs_limit) and                            \
                                 (best_epoch_name in array(curve_data[2])[:,0]):
        ra = curve_data[0]
        dec = curve_data[1]
        # curve = curve_data[2]
        # phot_cat, MAG_PSF, MAGERR_PSF, MAG_APER[20], MAGERR_APER[20]
        phot_cat_list = []
        mag_psf_list = []
        magerr_psf_list = []
        mag_aper_list = []
        magerr_aper_list = []
        best_epoch_mag_psf = 0
        best_epoch_mag_aper = 0
        for epoch in curve_data[2]:
            if epoch[0] == best_epoch_name:
                best_epoch_mag_psf = epoch[1]
                best_epoch_mag_aper = epoch[3]
                phot_cat_list.append(epoch[0])
                mag_psf_list.append(epoch[1])
                magerr_psf_list.append(epoch[2])
                mag_aper_list.append(epoch[3])
                magerr_aper_list.append(epoch[4])
            else:
                for matched_epoch in epoch_psf_izp_list:
                    if epoch[0] == matched_epoch[0]:
                        phot_cat_list.append(epoch[0])
                        # m_calib = m_inst - zp
                        mag_psf_list.append(epoch[1] - matched_epoch[1])
                        magerr_psf_list.append(epoch[2])
                for matched_epoch in epoch_aper_izp_list:
                    if epoch[0] == matched_epoch[0]:
                        mag_aper_list.append(epoch[3] - matched_epoch[2])
                        magerr_aper_list.append(epoch[4])
        
        psf_mag_medval, psf_mag_sigma = mad_clipping(array(mag_psf_list), 3) 
        aper_mag_medval, aper_mag_sigma = mad_clipping(array(mag_aper_list), 3)
        
        # Calculate the number of outliers from the "constant" brightness.
        # This is how we reject the variable candidate calibrators.
        num_mag_psf_outliers = len(array(mag_psf_list)[                        \
            (array(mag_psf_list)<psf_mag_medval-3*psf_mag_sigma) |             \
            (array(mag_psf_list)>psf_mag_medval+3*psf_mag_sigma)])
        num_mag_aper_outliers = len(array(mag_aper_list)[                      \
            (array(mag_aper_list)<aper_mag_medval-3*aper_mag_sigma) |          \
            (array(mag_aper_list)>aper_mag_medval+3*aper_mag_sigma)])
        
        # Now fill in the final zeropoint dictionaries.
        if (psf_mag_sigma < 0.075) and (aper_mag_sigma < 0.075) and          \
                            num_mag_psf_outliers<3 and num_mag_aper_outliers<3:        
            psf_mag_sigma_list.append(psf_mag_sigma)
            aper_mag_sigma_list.append(aper_mag_sigma)
            psf_mag_medval_list.append(psf_mag_medval)
            aper_mag_medval_list.append(aper_mag_medval)
            for epoch in curve_data[2]:
                # zp = m_inst - m_calib
                psf_photometry_zp_dict[epoch[0]].append(epoch[1] -             \
                                                        best_epoch_mag_psf)
                aper_photometry_zp_dict[epoch[0]].append(epoch[3] -            \
                                                        best_epoch_mag_aper)
            output_region_file.write(('''circle(%f,%f,1") # color=blue ''' +   \
                    "width=1 text={BEST_MAG_PSF=%.3f}\n") % (ra, dec,          \
                                                            best_epoch_mag_psf))
            calibrator_ra.append(ra)
            calibrator_dec.append(dec)
output_region_file.close()
calibrator_coords = array(zip(calibrator_ra, calibrator_dec))



for image_path in image_path_list:
    source_extractor_data = loadtxt(image_path.replace(".fits", ".sexcat"))
    # Keep only the non-flagged, good detections.
    #       source_extractor_data[:,29] is the FLAGS column
    #       source_extractor_data[:,21] is the MAG_PSF column
    #       source_extractor_data[:,22] is the MAGERR_PSF column
    sex_ra = source_extractor_data[:,3][(source_extractor_data[:,29]==0) &     \
                                        (source_extractor_data[:,5]!=0)]
    sex_dec = source_extractor_data[:,4][(source_extractor_data[:,29]==0) &    \
                                         (source_extractor_data[:,5]!=0)]
    sex_mag_psf = source_extractor_data[:,21][(source_extractor_data[:,29]==0) \
                                            & (source_extractor_data[:,5]!=0)]
    sex_magerr_psf = source_extractor_data[:,22][                              \
             (source_extractor_data[:,29]==0) & (source_extractor_data[:,5]!=0)]

    output_calibrator_file = file(image_path.replace(".fits",                  \
                                                       ".calibrators.txt"), "w")
    output_region_file = file(image_path.replace(".fits", ".calibrators.reg"), \
                                                                            "w")
    output_region_file.write("global color=blue dashlist=8 3 width=1 " + 
        "font='helvetica 12 bold' select=1 highlite=1 dash=0 fixed=0 edit=1 " + 
        "move=1 delete=1 include=1 source=1\nfk5\n")

    for n in range(len(sex_ra)):
        star_ra = sex_ra[n]
        star_dec = sex_dec[n]
        
        distance_array = 3600*hypot(cos(star_dec*0.01745329252)*(star_ra-calibrator_coords[:,0]),            \
                                    star_dec-calibrator_coords[:,1])
        # if this detected star is within 2 arcsec of a identified calibrator,
        # then write it out
        if distance_array.min() < 2:         
            star_psfmag_z = sex_mag_psf[n]
            star_psfmagerr_z = sex_magerr_psf[n]
            output_region_file.write(('''circle(%f,%f,1") # color=green ''' + 
                "width=1 text={MAG_PSF=%.3f +/- %.3f}\n") % 
                (star_ra, star_dec, star_psfmag_z, star_psfmagerr_z))
            
            output_calibrator_file.write("%.7f\t%.7f\t%.4f\t%.4f\n" %          \
                (star_ra, star_dec, star_psfmag_z, star_psfmagerr_z))
            
    output_region_file.close()
    output_calibrator_file.close()





# Collect the final zeropoint data by epoch and calculate the median and 
# associated sigma.
epoch_psf_zp_list = []
epoch_aper_zp_list = []
for image_path in image_path_list:
    phot_cat_path = image_path.replace(".fits", ".sexcat")
    if phot_cat_path != best_epoch_name:
        psf_zp_array = array(psf_photometry_zp_dict[phot_cat_path])
        aper_zp_array = array(aper_photometry_zp_dict[phot_cat_path])
        psf_zp_medval, psf_zp_sigma, psf_zp_num_calibrators =                  \
                               mad_clipping(psf_zp_array, 3, return_length=True)
        aper_zp_medval, aper_zp_sigma, aper_zp_num_calibrators =               \
                              mad_clipping(aper_zp_array, 3, return_length=True)
        epoch_psf_zp_list.append([phot_cat_path, psf_zp_medval, 
                                  psf_zp_sigma/(psf_zp_num_calibrators**0.5)])
        epoch_aper_zp_list.append([phot_cat_path, aper_zp_medval, 
                                  aper_zp_sigma/(aper_zp_num_calibrators**0.5)])
    else:
        epoch_psf_zp_list.append([phot_cat_path, 0, 0])
        epoch_aper_zp_list.append([phot_cat_path, 0, 0])

# Create swarp input lists by rejecting the images for which an abnormally large
# zp offset is needed
epoch_psf_zp_vals = array(epoch_psf_zp_list)[:,1].astype(float)
epoch_psf_zp_vals_medval, epoch_psf_zp_vals_sigma =                            \
                                              mad_clipping(epoch_psf_zp_vals, 3)
epoch_aper_zp_vals = array(epoch_aper_zp_list)[:,1].astype(float)
epoch_aper_zp_vals_medval, epoch_aper_zp_vals_sigma =                          \
                                             mad_clipping(epoch_aper_zp_vals, 3)

# Reject the poor quality images from the list of images to coadd with swarp.
cat_names_array = array(epoch_psf_zp_list)[:,0].astype(str)
swarp_input_cats = cat_names_array[
 (epoch_psf_zp_vals<epoch_psf_zp_vals_medval+3*epoch_psf_zp_vals_sigma) & 
 ((epoch_psf_zp_vals>epoch_psf_zp_vals_medval-3*epoch_psf_zp_vals_sigma) &
 (epoch_aper_zp_vals<epoch_aper_zp_vals_medval+3*epoch_aper_zp_vals_sigma) & 
 ((epoch_aper_zp_vals>epoch_aper_zp_vals_medval-3*epoch_aper_zp_vals_sigma)))]



# Write out the list of images to coadd with swarp.
swarp_input_list = file(coadd_directory + query_field + "-" + query_chip +     \
                        "_swarp_list.txt", "w")
for cat_name in swarp_input_cats:
    swarp_input_list.write(cat_name.replace(".sexcat", ".fits") + "\n")
swarp_input_list.close()

# Write out a file of the PSF and aperture photometry relative zeropoints that
# were used for each file (to compare with nightly solutions of absolute 
# photometry outputs).
relative_zp_output_file = file(coadd_directory + query_field + "-" +           \
                        query_chip + "_relZP_list.txt", "w")
for cat_name in swarp_input_cats:
    
    hdulist = pyfits.open(cat_name.replace(".sexcat", ".fits"))
    airmass = str(hdulist[0].header["airmass"])
    obsdate = hdulist[0].header["DATE-OBS"]
    hdulist.close()
    
    
    relative_zp_output_file.write(cat_name.replace(".sexcat", ".fits") + "\t")
    relative_zp_output_file.write(airmass + "\t" + obsdate + "\t")
    
    for epoch_zp_entry in epoch_psf_zp_list:
        if epoch_zp_entry[0] == cat_name:
            relative_zp_output_file.write("%.5f\t%.5f\t" %                     \
                (epoch_zp_entry[1], epoch_zp_entry[2]))
    for epoch_zp_entry in epoch_aper_zp_list:
        if epoch_zp_entry[0] == cat_name:
            relative_zp_output_file.write("%.5f\t%.5f" %                     \
                (epoch_zp_entry[1], epoch_zp_entry[2]))
    relative_zp_output_file.write("\n")
relative_zp_output_file.close()

for psf_zp_data in epoch_psf_zp_list:
    image_name = psf_zp_data[0].replace(".sexcat", ".fits")
    psf_zp = psf_zp_data[1]
    # m_calib = m_inst - zp
    psf_scale_factor = 10**((psf_zp)/2.5)
    system("sethead " + image_name + " FLXSCA-P=" + str(psf_scale_factor))

for aper_zp_data in epoch_aper_zp_list:
    image_name = aper_zp_data[0].replace(".sexcat", ".fits")
    aper_zp = aper_zp_data[1]
    # m_calib = m_inst - zp
    aper_scale_factor = 10**((aper_zp)/2.5)
    system("sethead " + image_name + " FLXSCA-A=" + str(aper_scale_factor))

# Create the coadded images:
system(SWARPCMD + " @" + coadd_directory + query_field + "-" + query_chip +                      
    "_swarp_list.txt -c rrl_combine.swarp -FSCALE_KEYWORD FLXSCA-P" +          
    " -WEIGHT_TYPE MAP_WEIGHT -WEIGHT_IMAGE " + weightmap_image +               
    " -IMAGEOUT_NAME " + coadd_directory + query_field + "-" +
     query_chip + "_psf_coadd.fits" +  
    " -WEIGHTOUT_NAME " + coadd_directory + query_field + "-" + query_chip +                     
    "_psf_coadd.weight.fits")
# system(SWARPCMD + " @" + coadd_directory + query_field + "-" + query_chip +                      
#     "_swarp_list.txt -c rrl_combine.swarp -FSCALE_KEYWORD FLXSCA-A" +          
#     " -WEIGHT_TYPE MAP_WEIGHT -WEIGHT_IMAGE " + weightmap_image +              
#     " -IMAGEOUT_NAME " + coadd_directory + query_field + "-" + 
#     query_chip + "_aper_coadd.fits" + 
#     " -WEIGHTOUT_NAME  " + coadd_directory + query_field + "-" + query_chip +                    
#     "_aper_coadd.weight.fits")

end_time = time.time()
print "Successfully completed coaddition of " + query_field + "-" +            \
    query_chip + "\t" + str(round(end_time-start_time)) + " seconds required."
